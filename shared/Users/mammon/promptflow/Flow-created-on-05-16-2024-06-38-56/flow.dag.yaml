id: template_chat_flow
name: Template Chat Flow
inputs:
  chat_history:
    type: list
    is_chat_input: false
    is_chat_history: true
  question:
    type: string
    is_chat_input: true
outputs:
  answer:
    type: string
    reference: ${chat.output}
    is_chat_output: true
nodes:
- name: chat
  use_variants: true
node_variants:
  chat:
    default_variant_id: variant_0
    variants:
      variant_0:
        node:
          name: chat
          type: llm
          source:
            type: code
            path: chat.jinja2
          inputs:
            deployment_name: gpt-4-3
            temperature: 0.7
            top_p: 0.95
            stop: []
            max_tokens: 1500
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            question: ${inputs.question}
            chat_history: ${inputs.chat_history}
          provider: AzureOpenAI
          connection: watsonx
          api: chat
          module: promptflow.tools.aoai
          activate:
            when: ${inputs.question}
            is: true
      variant_1:
        node:
          name: chat
          type: llm
          source:
            type: code
            path: chat__variant_1.jinja2
          inputs:
            deployment_name: gpt-4-3
            temperature: 0.7
            top_p: 0.95
            stop: []
            max_tokens: 1500
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: watsonx
          api: chat
          module: promptflow.tools.aoai
          activate:
            when: ${inputs.question}
            is: true
      variant_2:
        node:
          name: chat
          type: llm
          source:
            type: code
            path: chat__variant_2.jinja2
          inputs:
            deployment_name: gpt-4-3
            temperature: 0.7
            top_p: 0.95
            stop: []
            max_tokens: 1500
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: watsonx
          api: chat
          module: promptflow.tools.aoai
          activate:
            when: ${inputs.question}
            is: true
      variant_3:
        node:
          name: chat
          type: llm
          source:
            type: code
            path: chat__variant_3.jinja2
          inputs:
            deployment_name: gpt-4-3
            temperature: 0.7
            top_p: 0.95
            stop: []
            max_tokens: 1500
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: watsonx
          api: chat
          module: promptflow.tools.aoai
          activate:
            when: ${inputs.question}
            is: true
      variant_4:
        node:
          name: chat
          type: llm
          source:
            type: code
            path: chat__variant_4.jinja2
          inputs:
            deployment_name: gpt-4-3
            temperature: 0.7
            top_p: 0.95
            stop: []
            max_tokens: 1500
            response_format:
              type: text
            presence_penalty: 0
            frequency_penalty: 0
            chat_history: ${inputs.chat_history}
            question: ${inputs.question}
          provider: AzureOpenAI
          connection: watsonx
          api: chat
          module: promptflow.tools.aoai
          activate:
            when: ${inputs.question}
            is: true
environment:
  python_requirements_txt: requirements.txt
